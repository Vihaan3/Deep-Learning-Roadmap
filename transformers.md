# Transformers

https://www.youtube.com/watch?v=kCc8FmEb1nY

* [ ] Do the standard playing around for this
* [ ] Understand the difference between encoder and decoder, self-attention vs cross attention, layernorm and batchnorm
* [ ] Search up more about residual connections to get more intuition for why they actually work. You might get this right away, but for me it felt kind of unintuitive but really satisfying when I gained a mental model for it.

Intuition question: Explain attention from the perspective of two concurrent tokens _a_ and _b_ where a = "D" and b = "urs" and _b_ is attending to _a_.

[https://arena3-chapter1-transformer-interp.streamlit.app/%5B1.1%5D\_Transformer\_from\_Scratch](https://arena3-chapter1-transformer-interp.streamlit.app/\[1.1]\_Transformer\_from\_Scratch)\
\- speedrun through this again and add some comments on what exercises to do and what not to do. Add intuition questions. \
\
Do the rest of mech interp too, if you find it interesting!
